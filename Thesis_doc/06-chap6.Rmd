# Results

The following section presents the results of fitting five different models for classficiation on the original feature space as well as the first 11 principal components to account for 85% of the variance. This was done as there are many features that measure aspects of the same thing, i.e. there are seven different features measuring aspects of scale degree use where each scale degree can individually have interesting musical interpretations.

The five classifiers were chosen as they were used previously in the literature for similar problems (KNN, Naive Bayes and Random forests), or are popular models for classification (Logistic, LDA). For the logistic classifier we used lasso as a shrinkage method.

Each model (10 total) was tested for predictive accuracy by finding the $5-$fold cross-validated misclassification rate. The cross-validated misclassification rate showed a lot of variance depending on the random fold assigned, so the process was repeated 100 times, and the rate was averaged. 

## Models Results - Bach/Mendelssohn


Model        | misclass. rate - features | misclass. rate -  11 PCs 
-------------|---------------------------|-------------------------------
Logistic     | 0.078                     | 0.097
LDA          | 0.092                     | 0.049
KNN  (K = 9) | 0.097                     | 0.098 
Naive Bayes  | 0.103                     | 0.112
Random forest| 0.058                     | 0.111

Table: Averaged 5-fold CV misclassification rates for each model from 100 runs on the feature space and the first 11 PCs. 


Table 6.1 shows the averaged 5-fold cross-validated misclassification rates over 100 runs. For the KNN case, the features were scaled and centered first. Most models have misclassification rates round 10%. The best model was found to be linear discriminant analysis run on the principal components. 


 Figure 6.1 shows coefficient values for each feature for varying $\log(\lambda)$ values. $\lambda$ corresponds the restriction penalty. Higher values of $\log(\lambda)$ correspond to a higher penalty, resulting in coefficient estimates that shrink to zero. The cross-validated lasso logistic fit chose $\log(\lambda) = -6.2$ to have the lowest misclassification rate. At that value, most features besides the third and fifth scale degrees and frequency of eighth note are nonzero. We have that scale degree two and mean density features have low coefficient estimates at this point, but are nonzero, Overall, for all $\lambda$ penalties, it appears that the features for density and frequency of 16th note rhythms stay non-zero the longest, implying these features are more useful features for distinguishing the composer.

<!--
Figure 6.1 shows the misclassification rates corresponding to different values of $\log(\lambda)$ in the logistic lasso model. As the $\log(\lambda)$ increases, the coefficients of each of the features start approaching zero. They do this at different times and rates.

The left dotted line represents the minimum lambda, and the right line represents the lambda within one standard deviation. It is a more restricted model that can guard against over fitting, but is not used here. --> 

\begin{figure}[H]
\centering
\includegraphics[scale = .6]{images/lasso_coef_b.pdf}
\caption{Bach/Mendelssohn lasso logistic regression coefficients for changing $\lambda$ penalty values}
\label{subd}
\end{figure}

<!--
\begin{figure}[h]
\centering
\includegraphics[scale = .5]{images/loglambda_b.pdf}
\caption{Cross-validated misclassification rates for different lambdas}
\label{subd}
\end{figure}
-->

<!--
We also fit a lasso logistic model on the first 11 principal components. This resulted in an optimal $\log(\lambda)$ of $-3.8$, and a resulting misclassification rate of 0.097. -->

\begin{figure}[H]
\centering
\includegraphics[scale = .5]{images/var_imp_rflog_f.pdf}
\caption{Random forest variable importance and logistic regression coefficients and best $\lambda$ Bach/Mendelssohn}
\label{subd}
\end{figure}

Figure 6.2 shows the variable importance rankings as a mean decrease in the Gini index for the random forest model. It also shows the absolute value of the logistic regression coefficients at the best value for $\lambda$. Note that we look at the relative height of the bars here to determine importance. We can see that while the random forest does not view frequencies of dotted quarter notes to be a very useful predictor, the logistic model views it as very important. The density features are the most important in deciding splits of the trees, although these values do not have high coefficients in the logistic. Both models agree that the last seven variables are not important. They were shrunk to zero in the logistic lasso model. 



<!-- =========================================================== -->
<!-- =========================================================== -->
<!-- =========================================================== -->

## Model fit Felix/Fanny

Model        | misclass. rate - features | avg. misclass. rate -  11 PCs 
-------------|---------------------------|-------------------------------
Logistic     | 0.467                     | 0.45
LDA          | 0.518                     | 0.505
KNN          | 0.498                     | 0.467
Naive Bayes  | 0.502                     | 0.521
Random forest| 0.535                     | 0.571

Table: Averaged 5-fold CV misclassification rates of 100 runs on the feature space and the first 11 principal components for each model. 

Table 6.2 shows all the misclassification rates of each model. Unfortunately most of the misclassification rates are above 0.5, indicating that random guessing would perform better at predicting the composer. A logistic lasso classification model fit on the principal components performed best, with a misclassification rate of 0.45. <!-- The $\log(\lambda)$ penalty associated with this rate was -3.21. -->

\begin{figure}[H]
\centering
\includegraphics[scale = .6]{images/loglambda_f.pdf}
\caption{Fanny/Felix lasso logistic regression coefficients for changing $\lambda$ penalty values}
\label{subd}
\end{figure}

Figure 6.3 shows the coefficient estimates for varying $\log(\lambda)$ penalties for a logistic lasso model fit on the feature space. The frequency of the first scale degree appears to remain non-zero for the longest time. At $\log(\lambda)$ of -2.67 we get the best lambda fit. At this $\log(\lambda)$ value we have most of the coefficients shunk to zero besides frequency of thirty second notes and dotted eighth notes, and frequency of the first scale degree. 

<!--
\begin{figure}[H]
\centering
\includegraphics[scale = .5]{images/loglambda_f.pdf}
\caption{Cross-validated misclassification rates for different lambdas}
\label{subd}
\end{figure} 
Figure X shows the misclassification rates for varying values of log(lambda). Most rates are well above 0.5 (the error for a coin toss), although at log(lambda) of -2.8 we have the lowest misclassification error of 0.517.
When fit on the first eleven principal components, we have a lower misclassification rate. Figure X shows the misclassification errors for varying log(lambda). At -3.21 we have the lowest misclassification rate. 
\begin{figure}[H]
\centering
\includegraphics[scale = .6]{images/pca_loglambda_f.pdf}
\caption{Cross-validated misclassification rates for different lambdas}
\label{subd}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale = .5]{images/varImp_f.pdf}
\caption{Variable importance for a random forest model Fanny/Felix}
\label{subd}
\end{figure}-->

\begin{figure}[H]
\centering
\includegraphics[scale = .5]{images/var_imp_rflog_b.pdf}
\caption{Random forest variable importance and logistic regression coefficients and best $\lambda$ Fanny/Felix}
\label{subd}
\end{figure}

Figure 6.4 shows the variable importance used in random forests and the logistic regression coefficients at the best $\lambda$ value. At the best lambda value, only nine of the coefficients of logistic regression are non-zero. The variables that the random forest model found useful did not correspond very well to the variables remaining non-zero in the logistic lasso model. 

<!-- =========================================================== -->
<!-- =========================================================== -->
<!-- =========================================================== -->

## Predictions for Disputed pieces: 

Since we have such high misclassification rates using each model, the following predictions are likely not accurate. Table 6.3 shows the predicted classification for each piece for each model. 

Model            | Op.8 2 | Op.8 3 | Op.8 12 | Op.9 7 | Op.9 10 | Op.9 12
-----------------|--------|--------|---------|--------|---------|--------
logistic-lasso   | fanny  | fanny  | felix   | fanny  | felix   |fanny
LDA              | fanny  | fanny  | fanny   | felix  | felix   |fanny
KNN              | felix  | fanny  | felix   | felix  | fanny   |felix
Naive Bayes      | fanny  | fanny  | fanny   | fanny  | fanny   |fanny
Random Forest    | felix  | felix  | felix   | fanny  | fanny   |felix

Table: Predicted composer for each disputed piece
  