# About the Models

For classification, we are interested in using a set of features to predict the response, or composer. The notation used in this chapter is inspired by *The Elements of Statistical Learning* [@esl] and *An Introduction to Statistical Learning* [@isl]. Our features space $X$ is an $n \times p$ matrix, where $n$ is the size of our data, and $p$ is the number of predictors. Each $X_i$ is vector of values for a certain feature. $x_{ij}$ denotes the $i^{th}$ values of the $j^{th}$ feature. Each song has a composer or response, known or unknown, denoted by $Y$. In our case we have $Y \in \{\text{Fanny},\text{Felix}, \text{Bach}\}$, or more generally, $y \in \{\text{list of composers}\}$. The composer takes values in a discrete set, which is the possible composers. Thus we can always divide the input space into a collection of regions labeled according to the classification 

### Linear Methods for Classification 
### Linear Regression: 
A naive model for classification is linear regression. If our predictor space $Y$ has $K$ classes, we code the response $K$ different indicator responses $y_k$ where $y_k = 1$ if $Y = k$ and 0 otherwise. We can use the resulting $k$ hyperplanes as a decision boundary. We  find the coefficients for the line by finding coefficients $\beta$ to minimize the residual sum of squares. 

$$ RSS(\beta) = (\textbf{y} - \textbf{X}\beta)^T(\textbf{y} - \textbf{X}\beta) $$

where $\textbf{X}$ is an $N\times p$ matrix with each row an input vector, and $\textbf{y}$ is an $N$-vector of the outputs of the training set. 
This gives the unique solution: 

$$ \hat{\beta} = (\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}$$

This gives us; 

$$\hat{\textbf{Y}} = \textbf{X}(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T
\textbf{Y}$$
If there are $K$ classes, and we have that the fitted linear model for the $kth$ class is $\hat{f}_k(x) = \hat{\beta}_{k0} + \hat{\beta}^T_kx$, the decision boundary between class $k$ and class $l$ is the set of points for which $\hat{f}_k(x) = \hat{f}_l(x)$ which is equivalent to the set $\{x: (\hat{\beta}_{k0} - \hat{l0}) + (\hat{\beta}_k - \hat{\beta}_l)^Tx = 0\}$   which is the hyperplane. 

#### Linear Discriminant Analysis

Linear regression on a categorical variable that has multiple variables has issues when there isn't a natural ordering with the categories. For large $K$ and small $p$, groups can be masked.  When there is a binary response, we can calculate $P(Y|X)$, but linear regression can give predictions that aren't valid probabilities, namely negative probabilities or probabilities greater than 1. 

Knowing the class posteriors $P(Y = k|X)$ gives us an optimal classification. If we assume $f_k(x)$ is the class-conditional density of $X$ in class $G = k$ and that $\pi_k$ is the prior probability of class $k$ with $\sum_{k=1}^K \pi_k = 1$. We can then model $P(Y = k | X)$ by modeling the distribution of the features $X$ separately in each response class, and then use Bayes' theorem to calculate $P(Y = k |X)$ which gives us the following: 

$$ P(Y = k | X = x) = \frac{f_k(x)\pi_k}{\sum_{l = 1}^Kf_l(x)\pi_l}$$

We thus must have a model to find $f_k(x)$. Linear and quadratic discriminant analysis assume a multivariate Gaussian density, given by: $$f_k(x) = \frac{1}{(2\pi)^{p/2}|\mathbf{\Sigma}_k^{1/2}}e^{-\frac{1}{2}(x-\mu_k)^T\mathbf{\Sigma}_k^{-1}(x - \mu_k)}$$

Linear discriminant analysis (LDA) assumes that the covariance matrix is equal for every $k$: $\mathbf{\Sigma}_k = \mathbf{\Sigma} \forall k$. Quadratic discriminant analysis does not have this assumption. In addition we assume $\hat{\pi}_k = N_k/N$ where $N_k$ is the number of class - $k$ observations, $\hat{\mu}_k = \sum_{g_i = k}x_i/N_k$, and $\mathbf{\hat{\Sigma}} = \sum_{k = 1}^{K}\sum_{g_i = k}(x_i - \hat \mu_k)(x_i - \hat\mu_k)^T / (N- K)$

For LDA we can look at the log ratio comparing two classes $k$ and $l$ and can show:

$$\log\frac{P(Y= k|x = x)}{P(Y = l|X = x)} = \log\frac{f_k(x)}{f_l(x)} + \log\frac{\pi_k}{\pi_l} = \log \frac{\pi_k}{\pi_l} - \frac{1}{2}(\mu_k + \mu_l)^T\mathbf{\Sigma}^{-1}(\mu_k- \mu_l) + x^T\mathbf{\Sigma}^{-1}(\mu_k - \mu_l) $$

This is a linear equation, so the classes will be separated by hyperplanes. From the above, we can find that the predicted class for any $x$ is :

$$ \delta_k(x) = x^T\mathbf{\Sigma}^{-1}\mu_k - \frac{1}{2}\mu_k^T\mathbf{\Sigma}^{-1}\mu_k + \log \pi_k $$

These functions are known as $\textit{linear discriminant functions}$
We predict the class by finding the maximum value of the discriminant functions of all $k$. 

For QDA we get the following discriminant functions: 
$$ \delta_k(x) = -\frac{1}{2}\log|\mathbf{\Sigma}_k| - \frac{1}{2}(x - \mu_k)^T\mathbf{\Sigma}_k^{-1}(x - \mu_k) + \log \pi_k $$

Linear discriminant analysis is helpful when the classes are well separated, when $n$ is small and the distribution of the predictors $X$ is approximately normal in each of the classes, and when there are more than two response classes. 

#### Naive Bayes
The Naive Bayes classifier is often used for musical classification as it is good when the dimension $p$ of the features space is large. It makes the (naive) assumption that all the features are independent for a given class $i$, ie that $$f_i(X) = \prod_{k = 1}^p f_{ik}(X_k)$$. In practice this is not the case, but the model still preforms surprisingly well in practice when this assumption does not hold. 

#### Logistic Regression
Logistic regression differs from linear discriminant analysis by directly modeling $P(Y = k|X)$ by using the logistic function. The idea is to model the posterior probabilities of each of the $K$ classes as linear functions in $x$ and requiring that the probabilities sum to 1. The model has the form: 
$$ \log \bigg( \frac{p(X)}{1-p(X)} \bigg) = \beta_0 + \beta_1 X_1 + \cdots + \beta_pX_p$$

which can be written as: 

$$ p(X) = \frac{e^{\beta_0 + \beta_1X_1 + \cdots + \beta_pX_p}}{1 +e^{\beta_0 + \beta_1X_1 + \cdots + \beta_pX_p} }$$

We estimate the regression coefficients by using maximum likelihood. 
The log-likelihood for $N$ observations is: 

$$ \ell(\theta) = \sum_{i = 1}^N \log p_{g_i}(x_i;\theta)$$
where $p_k(x_i;\theta) = P(Y = k|X = x_i;\theta)$
We then choose $\theta$ to maximize this function.


#### K- nearest - neighbor (expand)

Another method for classification is k-nearest neighbor methods. It uses observations in the training set closest to $x$ to form $\hat{Y}$, the outputs. We often use Euclidean distance as a metric for closeness, although other methods exist. It is defined as
$$ \hat{Y}(x) = \frac{1}{k}\sum_{x_i \in N_k(x)}y_i$$

where $N_k(x)$ is the $k$ closest points, or neighbors, $x_i$ in the training set. This is equivalent to taking the average of $k$ observations with $x_i$ closest to $x$. This gives a predicted class of taking the mode of the $k$ nearest neighbors.

### Dimensionality Reduction

#### Principal component analysis

Principal component analysis (PCA) tranforms the features space into a lower dimensional representation. It choses the transformed features to have maximal variance and be mutually uncorrelated. 

Principal component analysis can be useful when the predictors are correlated. We suspect many of our features are correlated, due to certain patterns in music, as well as the way we created our features. These relationships are caused by similarity in the features, and from music theory rules. (For example, if there is a high frequency of first scale degrees, we might expect a high frequency of chords that include the first scale degree. Another example, if we had a high frequency of seventh scale degrees, we would expect them to resolve to the first scale degree.)

Principal component analysis is also helpful when there are many predictors, and we want to deal with a smaller dimension of predictor space. 

As an unsupervised method, PCA can inform about latent meta variables. 

Used in supervised methods, the transformed features from PCA can be used to fit models instead of the original features space. 

Principal components transforms the feature space. If our original features are $X_1,X_2,\ldots,X_p$, we transform the features to $Z_1,Z_2,\ldots,Z_M$, where $M < p$. Each $Z_i$ is a linear combination of the original predictors, ie, $Z_m = \sum_{j = 1}^p \phi_{jm}X_j$, for constants $\phi_{1m},\phi_{2m},\ldots,\phi_{pm}$ for $m = 1,\ldots,M$. Given an $n\times p$ data set $\mathbf{X}$ where $x_{ij}$ is the $i^{th}$ instance of the $j^{th}$ feature, we solve for the $m^{th}$ principal component loading vector  $\phi_m = \phi_{1m},\phi_{2m},\ldots,\phi_{pm}$ that solves the optimization
problem:  $$\max_{\phi_{1m},\ldots,\phi_{pm}} \bigg\{\frac{1}{n}\sum_{i=1}^n\bigg(\sum_{j=1}^p \phi_{jm}x_{ij}\bigg)^2\bigg\}$$,
where the $\phi$s are subject to $\sum_{j = 1}^p\phi_{jm}^2 = 1$. Our principal components are then calculated as $z_{im} = \sum_{j = 1}^p\phi{jm}x_{ij}$

The loadings of the first principal component, $\phi_1$ thus determine the direction in the feature space with the most variance, $Z_1$, or the scores of the first principal component is then a new feature in our transformed feature space. We continue calculating $Z_i$, where each following $Z_i$ has the maximal variance in a direction uncorrelated to the previous principal components. 

Before PCA is performed, we center all features to have mean zero, as the scale of some features are not the same, which will lead to issues in the loadings, as the features with higher scales would automatically have the higher variance. 

We can observe the proportion of variance explained by each principal component. This is usually visualized in a skree plot. We can use this information to decide how many principal components to use. 


### Model selection/assesment

#### 5-fold Cross validation (expand)

Cross validation involves fitting a model on a training set, and then using the fitted model to predict the responses in the testing set. $k$-fold cross validation involves splitting the data set into $k$ different parts of equal size, and fitting a model on the the data set with one of the $k$ parts withheld. The model is then tested on the witheld data. This is useful for model selection and determining the accuracy of the model. In this paper, as is common, we use $k = 5$


### Lasso model selection

The Lasso penalty was proposed by Robert Tibshirani in 1996. Lasso regression works by giving a penalty to regression coefficients. It essentially performes variable selection, as for high enough penalties, coeficcients shrink to zero. It is often used in linear regression, but can be expended to logistic regression and other generalized linear models. For linear regression, the lasso works by choosing coefficients $\beta_\lambda^L$ that minimize 
 $$ \sum_{i = 1}^n \bigg( y_i - \beta_0 - \sum_{j = 1}^p \beta_jx_{ij}\bigg)^2 + \lambda \sum_{j = 1}^p|\beta_j|$$
 We have that $\lambda$ is a tuning parameter. Increasing $\lambda$ will shrink the coefficients. This can be equivantely stated as:
 $$ \text{minimize}_\beta \bigg\{\sum_{i = 1}^n\bigg(y_i - \beta_0 - \sum_{j = 1}^p \beta_jx_{ij}\bigg) ^2 \text{ subject to } \sum_{j = 1}^p |\beta_j| \leq s$$
 
To expand to generalized linear models, if our model uses some parameter $\beta$ that was estimated by by some function $\ell(\beta)$ (where $\ell(\beta)$ is a log-likelyhood function for example), we maximize $\ell(\beta)$ subject to $\sum_{j=1}^p|\beta_j| < s$ [@lasso]


### Some trees? 


### K-Measns Clustering (plus other clusterings?)



