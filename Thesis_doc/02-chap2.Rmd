# 
##Summary of notation and vocabulary used in this paper

Vocab | description
----- | -----------
piece | the entirety of one opus, referred to as a piece in any stage
staff | one instrument composed of one staff line
etc ... 

## About the data and conversion process

### Pieces used
In total there were xxx pieces from Mendelssohn, and xx from Fanny, They were...



insert picture of score

### Optical music recognition
Sheet music as a form of data requires a lengthy process of conversion before being able to be used in any analysis. Simply scanning the scores into, say, a PDF, gives no musical semantics and can only be viewed on screen or printed on paper. Thus, the two main steps in reading in data from sheet music are: using optical music recognition software to transform physical scores into digital formats. The second step is to read the digital format in to R.


The scores used in this paper were obtained from physical copies available in the Reed music library. These scores were then scanned using software designed for optical music recognition (OMR). 
This method requires learning from graphical and textual information. The main things the software must pick up are the locations of bar lines, notes and rests, slurs, dynamic and tempo markings, lyrics etc. Basic optical music recognition has been around since 1966. 
Most commonly, the first step in optical music recognition is to remove the staff lines. The staff lines are critical, as they define the basis for the vertical definition distance of pitch, and the horizontal distance definition of rhythm. The staff gives a normalization that is helpful, essentially defining the size of what notes and Rhythm will look like. Staff removal methods include projections, histograms, run lengths, Candidates assemblage, contour tracking, and Graph path search. - which i don't understand and may not write about ... -  [@OMR]
The next step is music symbol extraction and classification. These methods include template matching, where the object in question is compared to existing known musical symbols, simple operators, such as analysis of bounding boxes and projections, joining graphical primatives, such as combining extracted objects such as notes, note heads, and note beams to connect them in a musically correct way to form chords etc. Other methods use statistical models for analyzing musical primitives (the objects its trying to classify) such as Neural Networks, Support Vector Machines, k-Nearest Neighbor, and Hidden Markov Models.

The next step is syntactical analysis and validation. This step essentially uses defined grammars describing the organization of music notation in terms of music symbols. This makes the classification problem simpler, as there are existing rules and relationships between musical symbols. 

insert picture of MuseScore score

The OMR used in this paper was PhotoScore. Photoscore outputs a musicXML file that can be read in by most music composing software, such as Sibelius, Finale or MuseScore. Muse score is a free software and was used in this paper. After being read into MuseScore, each piece was proof-read and corrected, as there were often errors in the OMR, especially in recognizing triplets. 


MusicXML on its own is not conducive to converting into a data frame as representing the single note middle c looks like this: 

```{}
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE score-partwise PUBLIC
    "-//Recordare//DTD MusicXML 0.5 Partwise//EN"
    "http://www.musicxml.org/dtds/partwise.dtd">
<score-partwise>
  <part-list>
    <score-part id="P1">
      <part-name>Music</part-name>
    </score-part>
  </part-list>
  <part id="P1">
     <measure number="1">
      <attributes>
        <divisions>1</divisions>
        <key>
          <fifths>0</fifths>
        </key>
        <time>
          <beats>4</beats>
          <beat-type>4</beat-type>
        </time>
        <clef>
          <sign>G</sign>
          <line>2</line>
        </clef>
      </attributes>
      <note>
        <pitch>
          <step>C</step>
          <octave>4</octave>
        </pitch>
        <duration>4</duration>
        <type>whole</type>
      </note>
    </measure>
  </part>
</score-partwise> 
```  

MusicXML is used commonly as it is conducive to representing sheet music and music notation, and it can be transferable to many different music softwares. 
We then need to convert into a format more easily readable into R. We do this by using Humdrum's function xml2hum that converts a musicXML file into a .krn file. (Talk about Humdrum) This file time can be read much more easily into R. Compared to above, the code for a single middle c whole note would be : 

```{}
**kern
*clefG2
*k[c]
*M4/4
=1-
1c/
```

The import_xml_files.sh file goes through the process of converting scores from musicXML to .krn. (insert stuff about online repositories of baroque and rennesaince composers already in this format) The files need to be separated into having a separate file for each staff, which would mean a seperate file for each instrument. In our case for each piece there are always two or three files for each piece, which are voice, piano right hand, and piano left hand. This is necessary to avoid the bugs in xml2hum that have issues when staffs don't necessarily match up as a result of the conversion process, most often by rhythm.

### .krn to R

Once we have .krn files to represent each piece (usually 2 or 3 files), we use regular expressions to extract key information. 

## Raw data frame: 

The data frame representing one piece: 
Each instrument contains the following columns: rhythm value [4], rhythm name [quarter note], note value [5], note name (octave inclusive)[cc], note name (octave exclusive)[ C sharp]

Additional column values are measure. 

Each row of the data frame contains one time base value. For a given piece, the time base represents the shortest note duration value. For example, if the shortest note a piece contained was a sixteenth note, the time base would be 16. Each measure then would contain 16 rows. 


## About the functions - Feature creation - liley some musical infromattion


Once the scores are converted into R raw data, feature creation begins. These features are mostly features suggested by John Cox. 





